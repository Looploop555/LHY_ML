## Reinforcement Learning，RL，强化学习

#### 1.什么是RL

 不知道正确答案是什么，难以标注，机器结合与外界的互动、知道什么样的输出是好的（Reward最大）

R：从某个时间点开始到结束的所有未来奖励的总和。

Agent：是学习如何通过执行动作来最大化某种累积奖励的实体。

Actor：负责决定采取什么行动。它基于当前的策略（Policy）来选择最合适的动作。Actor 的目标是通过调整其策略来增加获得的奖励。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730950432738.png" alt="1730950432738" style="zoom:50%;" />

 过程：（1）定义function

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730950860317.png" alt="1730950860317" style="zoom:50%;" />

（2）定义Loss

让R最大。（loss小，负loss就大）

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730951001866.png" alt="1730951001866" style="zoom:50%;" />

（3）优化器

让R越大越好，但Network、环境、R有**随机性**，

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730951685575.png" alt="1730951685575" style="zoom:50%;" />



----------------------------------------**-流程**----------------------------------------------

控制actor：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730952011610.png" alt="1730952011610" style="zoom:50%;" />

定义Loss：s是输入，a是行为

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730952072149.png" alt="1730952072149" style="zoom:50%;" />

重新定义Loss：A（Action）表示要做某个动作的意愿，e是交叉熵损失函数

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730952341550.png" alt="1730952341550" style="zoom:50%;" />





#### 2.Policy Gradient策略梯度

通过直接优化策略函数来学习最优行为策略

##### （1）概念

每一个行为都不是独立的，会影响接下来发生的事：
<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730953166229.png" alt="1730953166229" style="zoom:50%;" />

G是未来所有reward相加：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730970012670.png" alt="1730970012670" style="zoom:50%;" />

距离当前的a越近，reward的权重越达，越远reward的权重越小discount的方法：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730970168518.png" alt="1730970168518" style="zoom:50%;" />

reward的分数是相对的，可以做标准化：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730970449982.png" alt="1730970449982" style="zoom:50%;" />

##### （2）过程

每收集一个for循环的资料，才update一次参数；训练的actor和互动的actor最好是同一个：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730970646431.png" alt="1730970646431" style="zoom:50%;" />

off-policy:

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730971062822.png" alt="1730971062822" style="zoom:50%;" />

方法：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730971145100.png" alt="1730971145100" style="zoom: 50%;" />





#### 3.Actor-Critic

评估actor的好坏，在游戏结束之前，预估可能的G'是多少（s, situation/state），

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730971840712.png" alt="1730971840712" style="zoom:50%;" />

玩完整场游戏，才能得到训练value资料：V就是预测的G；

V（Θ）表示在策略Θ下从状态s开始的预期累计奖励。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730972011407.png" alt="1730972011407" style="zoom:50%;" />

不用玩完整场游戏，就能得到训练资料：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730972140008.png" alt="1730972140008" style="zoom:50%;" />

两种方法结果不一样：
<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730972586610.png" alt="1730972586610" style="zoom:50%;" />

同样的s，输出的actor不一定一样，因为有随机性。V就是把所有的G平均，Gt'是输入st后执行at输出的实际结果

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730974460908.png" alt="1730974460908" style="zoom:50%;" />

平均：rt是即时奖励，St+1是下一个状态，At在状态 s下选择动作 a 相对于期望值的优势？（在St的位置采取at，跳到St+1后得到的reward的期望值）（采取at这个action得到的reward的期望---不采取at、随机sample一个action得到reward）（rt+V(θ)（St+1）也是t时刻的reward，即第rt和rt后面reward的总和)
<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730974597452.png" alt="1730974597452" style="zoom:50%;" />

Actor和Critic各自是network：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730974838493.png" alt="1730974838493" style="zoom:50%;" />





#### 4.Reward Shaping回馈

sparse reward：稀疏回报，定义额外的Reward帮助Agent学习、让稀疏奖励更好被回报而设计的一种奖励的办法。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730990273788.png" alt="1730990273788" style="zoom: 80%;" />

Curiosity-based的Reward Shaping，机器活动的过程中看到有意义的、新的东西就加分。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1730990617308.png" alt="1730990617308" style="zoom: 80%;" />

#### 5.Imitation Learning

通过Actor与环境的互动、学习Expert的示范进行训练

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731064784531.png" alt="1731064784531" style="zoom:50%;" />

但机器可能遇到示例里没有的情况；机器不需要完全学习所有的动作，有些是个人特质，不需要学习。



#### 6.Inverse Reinforcement Learning（IRL）

从Expert的行为和与环境的互动反推Reward是什么，有了Reward，再去训练一个optimal actor（是能够根据最优策略（Optimal Policy）来选择动作的智能体（Agent））。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731065286185.png" alt="1731065286185" style="zoom:50%;" />

原则：老师总能得到比学生高的分数

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731065450668.png" alt="1731065450668" style="zoom:50%;" />

流程：

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731065488928.png" alt="1731065488928" style="zoom:50%;" />

Actor产生一个行为，Reward给Expert的行为高分，给Actor的行为低分，Actor要想办法得到高分。

<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731065643433.png" alt="1731065643433" style="zoom:50%;" />

例如：教机械手臂学习一些东西：
<img src="D:\Data\坚持就是胜利！\李宏毅机器学习\图片\17.Reinforcement Learning强化学习RL.assets\1731065871245.png" alt="1731065871245" style="zoom:50%;" />